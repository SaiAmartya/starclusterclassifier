# Purpose: Subcategorize star clusters using K-Means clustering
#          after classifying them as Open or Globular using Random Forest.

# Import necessary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from astropy.table import Table
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import os
from astropy import units as u
from astropy.coordinates import SkyCoord, Galactic, Galactocentric

# Configuration
input_filename = 'cleaned_cluster_members.fits'  # File generated by script 1
output_filename = 'cluster_subcategories.csv'  # Output file for subcategorization results

# Load the data
print(f"--- Loading Data from {input_filename} ---")
if not os.path.exists(input_filename):
    print(f"Error: Input file '{input_filename}' not found.")
    print("Please run '1_data_acquisition.py' first to generate the data.")
    exit()

try:
    combined_data = Table.read(input_filename, format='fits')
    if len(combined_data) == 0:
        print("Input file contains no data. Exiting.")
        exit()
    
    # Convert to pandas DataFrame for easier handling
    df_stars = combined_data.to_pandas()
    
    # Ensure correct types and handle potential missing values
    df_stars['radial_velocity'] = pd.to_numeric(df_stars['radial_velocity'], errors='coerce')
    df_stars['parallax'] = pd.to_numeric(df_stars['parallax'], errors='coerce')
    df_stars['pmra'] = pd.to_numeric(df_stars['pmra'], errors='coerce')
    df_stars['pmdec'] = pd.to_numeric(df_stars['pmdec'], errors='coerce')
    df_stars = df_stars.dropna(subset=['parallax', 'pmra', 'pmdec'])  # Need these for features

    print(f"Successfully loaded {len(df_stars)} star entries.")
    print(f"Clusters found: {df_stars['cluster_name'].unique()}")
except Exception as e:
    print(f"Error loading or processing data from '{input_filename}': {e}")
    exit()

# Calculate kinematic features (imported from 2_ml_classification.py)
def calculate_cluster_features(star_data):
    """
    Calculates cluster-level kinematic features from member star data.
    """
    import gala.potential as gp
    import gala.dynamics as gd
    from gala.units import galactic
    from astropy.coordinates import SkyCoord, Galactic, Galactocentric
    from astropy import units as u
    import gala.coordinates as gc
    
    cluster_features = []
    grouped = star_data.groupby('cluster_name')
    mw_potential = gp.MilkyWayPotential()  # Standard MW potential model

    for name, group in grouped:
        # Require minimum number of stars to calculate reliable dispersion
        if len(group) < 5:
            print(f"Skipping {name}: Not enough members ({len(group)}) for reliable stats.")
            continue

        print(f"Calculating features for {name} ({len(group)} members)...")
        features = {'cluster_name': name}
        features['cluster_type'] = group['cluster_type'].iloc[0]

        # Mean values
        mean_plx = group['parallax'].mean()
        mean_pmra = group['pmra'].mean()
        mean_pmdec = group['pmdec'].mean()
        mean_rv = group['radial_velocity'].mean()  # Returns NaN if all are NaN

        # Standard deviations (velocity dispersion)
        std_pmra = group['pmra'].std()
        std_pmdec = group['pmdec'].std()
        std_rv = group['radial_velocity'].std()  # Returns NaN if < 2 non-NaN values

        features['mean_parallax'] = mean_plx
        features['mean_pmra'] = mean_pmra
        features['mean_pmdec'] = mean_pmdec
        features['dispersion_pmra'] = std_pmra if pd.notna(std_pmra) else 0
        features['dispersion_pmdec'] = std_pmdec if pd.notna(std_pmdec) else 0
        features['dispersion_rv'] = std_rv if pd.notna(std_rv) else 0  # Use 0 if no RVs

        # Calculate 3D velocity dispersion in km/s
        distance_pc = 1000 / mean_plx if mean_plx > 0 else np.nan
        
        # Convert proper motion dispersions from mas/yr to km/s at cluster distance
        if pd.notna(distance_pc) and distance_pc > 0:
            # Convert mas/yr to km/s: 1 mas/yr at d parsecs ~= 4.74e-3 * d km/s
            pm_to_kms_factor = 4.74e-3 * distance_pc
            dispersion_pmra_kms = std_pmra * pm_to_kms_factor if pd.notna(std_pmra) else 0
            dispersion_pmdec_kms = std_pmdec * pm_to_kms_factor if pd.notna(std_pmdec) else 0
            
            # Use RV dispersion directly in km/s if available, otherwise 0
            dispersion_rv_kms = std_rv if pd.notna(std_rv) else 0
            
            # Calculate 3D dispersion
            dispersion_3d_kms = np.sqrt(dispersion_pmra_kms**2 + dispersion_pmdec_kms**2 + dispersion_rv_kms**2)
            features['dispersion_3d_kms'] = dispersion_3d_kms
        else:
            features['dispersion_3d_kms'] = np.nan

        # Calculate orbital parameters using gala
        try:
            # Create SkyCoord objects for all stars
            c = SkyCoord(
                ra=np.mean(group['ra']) * u.degree,
                dec=np.mean(group['dec']) * u.degree,
                distance=distance_pc * u.pc,
                pm_ra_cosdec=mean_pmra * u.mas/u.yr,
                pm_dec=mean_pmdec * u.mas/u.yr,
                radial_velocity=mean_rv * u.km/u.s if pd.notna(mean_rv) else 0 * u.km/u.s
            )
            
            # Transform to Galactocentric frame
            gc_frame = Galactocentric(galcen_distance=8.122*u.kpc, z_sun=0.0208*u.kpc)
            c_gc = c.transform_to(gc_frame)
            
            # Extract position and velocity components
            x = c_gc.x.to(u.kpc).value
            y = c_gc.y.to(u.kpc).value
            z = c_gc.z.to(u.kpc).value
            vx = c_gc.v_x.to(u.km/u.s).value
            vy = c_gc.v_y.to(u.km/u.s).value
            vz = c_gc.v_z.to(u.km/u.s).value
            
            # Convert to gala quantities
            pos = np.array([x, y, z]) * u.kpc
            vel = np.array([vx, vy, vz]) * u.km/u.s
            
            # Create orbit
            w0 = gd.PhaseSpacePosition(pos=pos, vel=vel)
            
            # Compute orbital energy directly from potential
            energy = mw_potential.energy(w0).to(u.km**2/u.s**2).value[0]
            features['orbital_energy'] = energy
            
            # Calculate angular momentum (particularly Lz component)
            L = np.cross(pos.value, vel.value)
            Lz = L[2] * u.kpc * u.km/u.s  # The z-component of angular momentum
            features['orbital_Lz'] = Lz.value
            
            # Calculate orbital eccentricity by integrating orbit
            orbit = mw_potential.integrate_orbit(w0, dt=1*u.Myr, n_steps=1000)
            
            # Calculate pericenter and apocenter
            r_orbit = np.sqrt(orbit.pos.x**2 + orbit.pos.y**2 + orbit.pos.z**2)
            rperi = np.min(r_orbit)
            rapo = np.max(r_orbit)
            
            if rperi > 0 and rapo > 0:
                ecc = (rapo - rperi) / (rapo + rperi)
                features['eccentricity'] = ecc.value
            else:
                features['eccentricity'] = np.nan
            
            # Add galactic position features that might be useful for classification
            features['galactic_x'] = x
            features['galactic_y'] = y
            features['galactic_z'] = z
            features['galactic_r'] = np.sqrt(x**2 + y**2 + z**2)  # Galactocentric distance
            
        except Exception as e:
            print(f"Error calculating orbital parameters for {name}: {e}")
            features['orbital_energy'] = np.nan
            features['orbital_Lz'] = np.nan
            features['eccentricity'] = np.nan
            features['galactic_x'] = np.nan
            features['galactic_y'] = np.nan
            features['galactic_z'] = np.nan
            features['galactic_r'] = np.nan
        
        cluster_features.append(features)
        
    df_features = pd.DataFrame(cluster_features)
    return df_features

# Calculate features
print("\n--- Starting Feature Engineering ---")
df_cluster_kinematics = calculate_cluster_features(df_stars)

# Handle potential NaN values produced during feature engineering
print("\nHandling potential NaN values in engineered features...")
cols_to_fill = ['dispersion_3d_kms', 'orbital_energy', 'orbital_Lz', 'eccentricity', 
               'galactic_x', 'galactic_y', 'galactic_z', 'galactic_r']

for col in cols_to_fill:
    if col in df_cluster_kinematics.columns:
        # Check if the column has any non-NaN values
        if not df_cluster_kinematics[col].isna().all():
            # Calculate median only if there are non-NaN values
            median_val = df_cluster_kinematics[col].median()
            print(f"Filling NaN values in {col} with median: {median_val}")
            df_cluster_kinematics[col].fillna(median_val, inplace=True)
        else:
            # If all values are NaN, fill with a default value
            print(f"All values in {col} are NaN. Filling with default value 0.")
            df_cluster_kinematics[col].fillna(0, inplace=True)

# Drop rows with missing values in critical features
critical_features_for_model = ['dispersion_3d_kms', 'orbital_energy', 'orbital_Lz']
df_cluster_kinematics = df_cluster_kinematics.dropna(subset=critical_features_for_model)

print("\n--- Feature Engineering Complete ---")
print(f"Generated features for {len(df_cluster_kinematics)} clusters.")
if not df_cluster_kinematics.empty:
    print(df_cluster_kinematics.head())
else:
    print("No clusters remaining after feature engineering. Exiting.")
    exit()

# --- Random Forest Classification (Open vs. Globular) ---
if len(df_cluster_kinematics) > 1:  # Need at least 2 samples for train/test split
    print("\n--- Preparing for Cluster Classification ---")

    # We'll use the original cluster types from the data instead of the prediction
    # because we want to subcategorize based on the true cluster types
    
    # Convert byte strings to regular strings if needed
    if df_cluster_kinematics['cluster_type'].dtype == object and isinstance(df_cluster_kinematics['cluster_type'].iloc[0], bytes):
        df_cluster_kinematics['cluster_type'] = df_cluster_kinematics['cluster_type'].apply(lambda x: x.decode('utf-8'))
    
    # Create predicted_type column directly from original cluster_type
    df_cluster_kinematics['predicted_type'] = df_cluster_kinematics['cluster_type'].apply(
        lambda x: 1 if x == 'Globular' else 0)
    df_cluster_kinematics['predicted_type_label'] = df_cluster_kinematics['cluster_type']
    
    print("\n--- Using Original Cluster Types ---")
    print(df_cluster_kinematics[['cluster_name', 'cluster_type', 'predicted_type_label']])
    
    # --- K-Means Clustering for Subcategories ---
    print("\n--- Performing K-Means Clustering for Subcategories ---")
    
    # Define features for subcategorization
    subcategory_features = {
        'Open': ['orbital_Lz', 'dispersion_3d_kms', 'eccentricity'],
        'Globular': ['orbital_Lz', 'dispersion_3d_kms', 'galactic_r']
    }
    
    # Dictionary to map cluster numbers to subcategory names
    open_cluster_labels = {
        0: 'Young Disk',
        1: 'Middle-Aged Disk',
        2: 'Old Disk'
    }
    
    globular_cluster_labels = {
        0: 'Disk/Bulge',
        1: 'Inner Halo',
        2: 'Outer Halo'
    }
    
    # Initialize subcategory column
    df_cluster_kinematics['subcategory'] = 'Unknown'
    
    # Separate open and globular clusters based on predicted types
    open_clusters = df_cluster_kinematics[df_cluster_kinematics['predicted_type'] == 0].copy()
    globular_clusters = df_cluster_kinematics[df_cluster_kinematics['predicted_type'] == 1].copy()
    
    # Function to apply K-Means and determine subcategories
    def apply_kmeans(df, features, n_clusters=3):
        if len(df) < n_clusters:
            print(f"Not enough clusters ({len(df)}) for K-Means with {n_clusters} clusters.")
            return df
        
        # Extract and scale features
        X = df[features].values
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        
        # Apply K-Means
        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
        df['subcategory_id'] = kmeans.fit_predict(X_scaled)
        
        # Calculate silhouette score if more than 1 cluster
        if len(df) > n_clusters and n_clusters > 1:
            silhouette_avg = silhouette_score(X_scaled, df['subcategory_id'])
            print(f"Silhouette Score: {silhouette_avg:.3f}")
        
        # Save cluster centers for later analysis
        centers = scaler.inverse_transform(kmeans.cluster_centers_)
        cluster_centers = pd.DataFrame(centers, columns=features)
        print("Cluster Centers:")
        print(cluster_centers)
        
        return df, centers, kmeans
    
    # Apply K-Means to Open Clusters
    if len(open_clusters) >= 3:  # Need at least 3 samples for 3 clusters
        print("\nSubcategorizing Open Clusters...")
        open_clusters, open_centers, open_kmeans = apply_kmeans(
            open_clusters, subcategory_features['Open'], n_clusters=3)
        
        # Interpret and assign meaningful labels to subcategories
        if len(open_centers) == 3:
            # Sort clusters by orbital_Lz (higher is younger typically)
            cluster_orbital_lz = {i: center[0] for i, center in enumerate(open_centers)}
            cluster_velocity_disp = {i: center[1] for i, center in enumerate(open_centers)}
            cluster_eccentricity = {i: center[2] for i, center in enumerate(open_centers)}
            
            # Determine which cluster is which subcategory based on orbital_Lz and velocity dispersion
            # Young Disk: High Lz, Low dispersion
            # Middle-Aged: Moderate Lz, Moderate dispersion
            # Old Disk: Low Lz, High dispersion
            
            # Create a combined score: high Lz and low dispersion = younger
            cluster_scores = {}
            for i in range(3):
                # Normalize Lz (higher is better)
                lz_normalized = (cluster_orbital_lz[i] - min(cluster_orbital_lz.values())) / \
                              (max(cluster_orbital_lz.values()) - min(cluster_orbital_lz.values()) + 1e-10)
                
                # Normalize dispersion (lower is better)
                disp_normalized = 1 - (cluster_velocity_disp[i] - min(cluster_velocity_disp.values())) / \
                                (max(cluster_velocity_disp.values()) - min(cluster_velocity_disp.values()) + 1e-10)
                
                # Normalize eccentricity (lower is better for young clusters)
                ecc_normalized = 1 - (cluster_eccentricity[i] - min(cluster_eccentricity.values())) / \
                               (max(cluster_eccentricity.values()) - min(cluster_eccentricity.values()) + 1e-10)
                
                # Combined score (higher = younger)
                cluster_scores[i] = lz_normalized + disp_normalized + ecc_normalized
            
            # Sort clusters by score
            sorted_clusters = sorted(cluster_scores.items(), key=lambda x: x[1], reverse=True)
            
            # Map cluster IDs to subcategories
            cluster_mapping = {
                sorted_clusters[0][0]: 'Young Disk',       # Highest score = youngest
                sorted_clusters[1][0]: 'Middle-Aged Disk', # Middle score = middle-aged
                sorted_clusters[2][0]: 'Old Disk'          # Lowest score = oldest
            }
            
            # Assign subcategories
            open_clusters['subcategory'] = open_clusters['subcategory_id'].map(cluster_mapping)
            
            print("\nOpen Cluster Subcategories:")
            print(open_clusters[['cluster_name', 'subcategory_id', 'subcategory']])
            
            # Update the main dataframe
            for idx, row in open_clusters.iterrows():
                df_cluster_kinematics.loc[idx, 'subcategory'] = row['subcategory']
                df_cluster_kinematics.loc[idx, 'subcategory_id'] = row['subcategory_id']
        
    else:
        print(f"Not enough Open clusters ({len(open_clusters)}) for K-Means subcategorization.")
    
    # Apply K-Means to Globular Clusters
    if len(globular_clusters) >= 3:  # Need at least 3 samples for 3 clusters
        print("\nSubcategorizing Globular Clusters...")
        globular_clusters, glob_centers, glob_kmeans = apply_kmeans(
            globular_clusters, subcategory_features['Globular'], n_clusters=3)
        
        # Interpret and assign meaningful labels to subcategories
        if len(glob_centers) == 3:
            # Sort clusters based on their characteristics
            cluster_orbital_lz = {i: center[0] for i, center in enumerate(glob_centers)}
            cluster_velocity_disp = {i: center[1] for i, center in enumerate(glob_centers)}
            cluster_galactic_r = {i: center[2] for i, center in enumerate(glob_centers)}
            
            # Categorize based on: 
            # Disk/Bulge: Higher Lz, Lower velocity dispersion, Smaller galactocentric distance
            # Inner Halo: Moderate values
            # Outer Halo: Low Lz, Very high velocity dispersion, Large galactocentric distance
            
            # Create a score where:
            # - Higher Lz contributes to disk/bulge classification
            # - Lower velocity dispersion contributes to disk/bulge classification
            # - Lower galactocentric distance contributes to disk/bulge classification
            
            cluster_scores = {}
            for i in range(3):
                # Normalize Lz (higher is more disk-like)
                lz_normalized = (cluster_orbital_lz[i] - min(cluster_orbital_lz.values())) / \
                              (max(cluster_orbital_lz.values()) - min(cluster_orbital_lz.values()) + 1e-10)
                
                # Normalize dispersion (lower is more disk-like)
                disp_normalized = 1 - (cluster_velocity_disp[i] - min(cluster_velocity_disp.values())) / \
                                (max(cluster_velocity_disp.values()) - min(cluster_velocity_disp.values()) + 1e-10)
                
                # Normalize galactic_r (lower is more disk/bulge-like)
                r_normalized = 1 - (cluster_galactic_r[i] - min(cluster_galactic_r.values())) / \
                             (max(cluster_galactic_r.values()) - min(cluster_galactic_r.values()) + 1e-10)
                
                # Combined score (higher = more disk/bulge-like)
                cluster_scores[i] = lz_normalized + disp_normalized + r_normalized
            
            # Sort clusters by score
            sorted_clusters = sorted(cluster_scores.items(), key=lambda x: x[1], reverse=True)
            
            # Map cluster IDs to subcategories
            cluster_mapping = {
                sorted_clusters[0][0]: 'Disk/Bulge',  # Highest score = disk/bulge
                sorted_clusters[1][0]: 'Inner Halo',  # Middle score = inner halo
                sorted_clusters[2][0]: 'Outer Halo'   # Lowest score = outer halo
            }
            
            # Assign subcategories
            globular_clusters['subcategory'] = globular_clusters['subcategory_id'].map(cluster_mapping)
            
            print("\nGlobular Cluster Subcategories:")
            print(globular_clusters[['cluster_name', 'subcategory_id', 'subcategory']])
            
            # Update the main dataframe
            for idx, row in globular_clusters.iterrows():
                df_cluster_kinematics.loc[idx, 'subcategory'] = row['subcategory']
                df_cluster_kinematics.loc[idx, 'subcategory_id'] = row['subcategory_id']
                
    else:
        print(f"Not enough Globular clusters ({len(globular_clusters)}) for K-Means subcategorization.")
    
    # --- Visualization ---
    print("\n--- Creating Visualizations ---")
    
    # Scatter plot of all clusters with subcategories
    plt.figure(figsize=(12, 8))
    
    # Define colors and markers for each cluster type
    colors = {
        'Young Disk': 'lightgreen',
        'Middle-Aged Disk': 'green', 
        'Old Disk': 'darkgreen',
        'Disk/Bulge': 'lightblue',
        'Inner Halo': 'blue',
        'Outer Halo': 'darkblue'
    }
    
    markers = {
        'Open': 'o',      # Circle for Open clusters
        'Globular': '^'   # Triangle for Globular clusters
    }
    
    # Create scatter plot
    for idx, row in df_cluster_kinematics.iterrows():
        if row['subcategory'] != 'Unknown':
            plt.scatter(
                row['orbital_Lz'], 
                row['dispersion_3d_kms'],
                color=colors.get(row['subcategory'], 'gray'),
                marker=markers.get(row['predicted_type_label'], 's'),  # Default to square if unknown
                s=100,
                alpha=0.7,
                edgecolors='black',
                label=f"{row['subcategory']} ({row['predicted_type_label']})"
            )
            
            # Add cluster name as text label
            plt.annotate(
                row['cluster_name'], 
                (row['orbital_Lz'], row['dispersion_3d_kms']),
                xytext=(5, 5),
                textcoords='offset points',
                fontsize=8
            )
    
    # Remove duplicate labels in legend
    handles, labels = plt.gca().get_legend_handles_labels()
    by_label = dict(zip(labels, handles))
    plt.legend(by_label.values(), by_label.keys(), loc='upper right')
    
    plt.xlabel('Orbital Angular Momentum (Lz)')
    plt.ylabel('3D Velocity Dispersion (km/s)')
    plt.title('Star Cluster Subcategories')
    plt.grid(True, alpha=0.3)
    plt.savefig('cluster_subcategories.png')
    
    # Create additional visualization for eccentricity vs. Lz
    plt.figure(figsize=(12, 8))
    
    for idx, row in df_cluster_kinematics.iterrows():
        if row['subcategory'] != 'Unknown':
            plt.scatter(
                row['orbital_Lz'], 
                row['eccentricity'],
                color=colors.get(row['subcategory'], 'gray'),
                marker=markers.get(row['predicted_type_label'], 's'),
                s=100,
                alpha=0.7,
                edgecolors='black',
                label=f"{row['subcategory']} ({row['predicted_type_label']})"
            )
            
            # Add cluster name as text label
            plt.annotate(
                row['cluster_name'], 
                (row['orbital_Lz'], row['eccentricity']),
                xytext=(5, 5),
                textcoords='offset points',
                fontsize=8
            )
    
    # Remove duplicate labels in legend
    handles, labels = plt.gca().get_legend_handles_labels()
    by_label = dict(zip(labels, handles))
    plt.legend(by_label.values(), by_label.keys(), loc='upper right')
    
    plt.xlabel('Orbital Angular Momentum (Lz)')
    plt.ylabel('Orbital Eccentricity')
    plt.title('Star Cluster Eccentricity vs. Angular Momentum')
    plt.grid(True, alpha=0.3)
    plt.savefig('cluster_eccentricity.png')
    
    # Save results to CSV
    print(f"\nSaving results to {output_filename}")
    df_cluster_kinematics.to_csv(output_filename, index=False)
    
    print("\nSubcategorization Complete!")
    print(f"Results saved to {output_filename}")
    print("Visualizations saved as 'cluster_subcategories.png' and 'cluster_eccentricity.png'")
    
else:
    print("\nCannot proceed with subcategorization: Not enough cluster data available (need at least 2).")

print("\nSubcategorization Script Finished.")
