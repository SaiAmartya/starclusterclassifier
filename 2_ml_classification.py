# Purpose: Load cleaned cluster member data, calculate kinematic features,
#          train a Random Forest model to classify clusters, and evaluate it.

# Import necessary libraries
import numpy as np
import pandas as pd
from astropy.table import Table
from astropy import units as u
from astropy.coordinates import SkyCoord, Galactic, Galactocentric  # Import Galactocentric from astropy.coordinates
import gala.coordinates as gc
import gala.potential as gp
import gala.dynamics as gd
from gala.units import galactic

# Machine Learning libraries
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.preprocessing import StandardScaler # Optional: Scale features
import matplotlib.pyplot as plt
import seaborn as sns
import os


# --- Configuration ---
input_filename = 'cleaned_cluster_members.fits' # File generated by script 1

# Define cluster types (label mapping)
cluster_type_mapping = {
    'Pleiades': 'Open',
    'Hyades': 'Open',
    'Praesepe': 'Open', 
    'M13': 'Globular',
    'M3': 'Globular',
    '47Tuc': 'Globular'
}


# --- Load Data ---
print(f"--- Loading Data from {input_filename} ---")
if not os.path.exists(input_filename):
    print(f"Error: Input file '{input_filename}' not found.")
    print("Please run '1_data_acquisition.py' first to generate the data.")
    exit() # Stop execution if input file is missing

try:
    combined_data = Table.read(input_filename, format='fits')
    if len(combined_data) == 0:
        print("Input file contains no data. Exiting.")
        exit()
    # Convert to pandas DataFrame for easier handling
    df_stars = combined_data.to_pandas()
    # Ensure correct types and handle potential missing values
    df_stars['radial_velocity'] = pd.to_numeric(df_stars['radial_velocity'], errors='coerce')
    df_stars['parallax'] = pd.to_numeric(df_stars['parallax'], errors='coerce')
    df_stars['pmra'] = pd.to_numeric(df_stars['pmra'], errors='coerce')
    df_stars['pmdec'] = pd.to_numeric(df_stars['pmdec'], errors='coerce')
    df_stars = df_stars.dropna(subset=['parallax', 'pmra', 'pmdec']) # Need these for features

    print(f"Successfully loaded {len(df_stars)} star entries.")
    print(f"Clusters found: {df_stars['cluster_name'].unique()}")
except Exception as e:
    print(f"Error loading or processing data from '{input_filename}': {e}")
    exit()


# --- 1. Feature Engineering ---
print("\n--- Starting Feature Engineering ---")

def calculate_cluster_features(star_data):
    """
    Calculates cluster-level kinematic features from member star data.
    """
    cluster_features = []
    grouped = star_data.groupby('cluster_name')
    mw_potential = gp.MilkyWayPotential() # Standard MW potential model

    for name, group in grouped:
        # Require minimum number of stars to calculate reliable dispersion
        if len(group) < 5:
             print(f"Skipping {name}: Not enough members ({len(group)}) for reliable stats.")
             continue

        print(f"Calculating features for {name} ({len(group)} members)...")
        features = {'cluster_name': name}
        features['cluster_type'] = group['cluster_type'].iloc[0]

        # Mean values
        mean_plx = group['parallax'].mean()
        mean_pmra = group['pmra'].mean()
        mean_pmdec = group['pmdec'].mean()
        mean_rv = group['radial_velocity'].mean() # Returns NaN if all are NaN

        # Standard deviations (velocity dispersion)
        std_pmra = group['pmra'].std()
        std_pmdec = group['pmdec'].std()
        std_rv = group['radial_velocity'].std() # Returns NaN if < 2 non-NaN values

        features['mean_parallax'] = mean_plx
        features['mean_pmra'] = mean_pmra
        features['mean_pmdec'] = mean_pmdec
        features['dispersion_pmra'] = std_pmra if pd.notna(std_pmra) else 0
        features['dispersion_pmdec'] = std_pmdec if pd.notna(std_pmdec) else 0
        features['dispersion_rv'] = std_rv if pd.notna(std_rv) else 0 # Use 0 if no RVs

        # Calculate 3D Velocity Dispersion in km/s
        disp_3d_kms = 0 # Default value
        if mean_plx > 0:
            distance_pc = 1000.0 / mean_plx
            # Factor 4.74 converts mas/yr * pc -> km/s
            disp_v_ra_kms = (std_pmra * distance_pc * 4.74) if pd.notna(std_pmra) else 0
            disp_v_dec_kms = (std_pmdec * distance_pc * 4.74) if pd.notna(std_pmdec) else 0
            features['dispersion_v_ra_kms'] = disp_v_ra_kms
            features['dispersion_v_dec_kms'] = disp_v_dec_kms
            disp_3d_sq = disp_v_ra_kms**2 + disp_v_dec_kms**2
            if pd.notna(std_rv) and std_rv > 0:
                disp_3d_sq += std_rv**2
            disp_3d_kms = np.sqrt(disp_3d_sq) if disp_3d_sq > 0 else 0
        else:
             features['dispersion_v_ra_kms'] = 0
             features['dispersion_v_dec_kms'] = 0
        features['dispersion_3d_kms'] = disp_3d_kms


        # Orbital Parameters (requires mean RV)
        features['orbital_energy'] = np.nan
        features['orbital_Lz'] = np.nan
        features['eccentricity'] = np.nan # Placeholder

        if mean_plx > 0 and pd.notna(mean_rv):
            mean_ra = group['ra'].mean()
            mean_dec = group['dec'].mean()
            try:
                sky_coord = SkyCoord(ra=mean_ra*u.deg, dec=mean_dec*u.deg,
                                     distance=distance_pc*u.pc,
                                     pm_ra_cosdec=mean_pmra*u.mas/u.yr,
                                     pm_dec=mean_pmdec*u.mas/u.yr,
                                     radial_velocity=mean_rv*u.km/u.s)
                galactocentric = sky_coord.transform_to(Galactocentric) # Use astropy's Galactocentric frame instead of gala's
                w0 = gd.PhaseSpacePosition(galactocentric.data)
                # Get energy from the potential
                E = mw_potential.energy(w0)[0].to(u.km**2/u.s**2).value
                
                # Calculate angular momentum manually using position and velocity
                # L = r Ã— p, where p = m*v and we set m=1
                pos = w0.pos.xyz.value
                vel = w0.vel.d_xyz.value
                L_x = pos[1] * vel[2] - pos[2] * vel[1]  # y*vz - z*vy
                L_y = pos[2] * vel[0] - pos[0] * vel[2]  # z*vx - x*vz
                L_z = pos[0] * vel[1] - pos[1] * vel[0]  # x*vy - y*vx
                
                features['orbital_energy'] = E
                features['orbital_Lz'] = L_z  # Use just Lz component (perpendicular to Galactic plane)
            except Exception as e:
                print(f"  - Could not calculate orbit for {name}: {e}")
        else:
             print(f"  - Skipping orbit calculation for {name} (missing parallax or RV).")

        cluster_features.append(features)

    return pd.DataFrame(cluster_features)

# Calculate features
df_cluster_kinematics = calculate_cluster_features(df_stars)

# Handle potential NaN values produced during feature engineering
# Simple strategy: fill with median of the column
print("\nHandling potential NaN values in engineered features...")
cols_to_fill = ['dispersion_3d_kms', 'orbital_energy', 'orbital_Lz', 'eccentricity']
for col in cols_to_fill:
     if col in df_cluster_kinematics.columns:
          # Always calculate the median value
          median_val = df_cluster_kinematics[col].median()
          # Fill NaN values if any
          if df_cluster_kinematics[col].isnull().any():
               df_cluster_kinematics[col] = df_cluster_kinematics[col].fillna(median_val)
               print(f"Filled NaNs in '{col}' with median: {median_val:.2f}")
          # Replace infinite values if any occur
          df_cluster_kinematics[col] = df_cluster_kinematics[col].replace([np.inf, -np.inf], median_val)

# Final check for rows with NaNs in critical features and drop if necessary
critical_features_for_model = ['dispersion_3d_kms', 'orbital_energy', 'orbital_Lz'] # Example
df_cluster_kinematics = df_cluster_kinematics.dropna(subset=critical_features_for_model)


print("\n--- Feature Engineering Complete ---")
print(f"Generated features for {len(df_cluster_kinematics)} clusters.")
if not df_cluster_kinematics.empty:
    print(df_cluster_kinematics.head())
else:
    print("No clusters remaining after feature engineering. Exiting.")
    exit()

# --- 2. Machine Learning Model ---
if len(df_cluster_kinematics) > 1: # Need at least 2 samples for train/test split
    print("\n--- Preparing for Machine Learning ---")

    # Define features (X) and target (y)
    feature_cols = [
        'dispersion_pmra', 'dispersion_pmdec', 'dispersion_rv',
        'dispersion_3d_kms',
        'orbital_energy', 'orbital_Lz'
        # Add 'eccentricity' if calculated and filled properly
    ]
    # Ensure selected feature columns exist and are numeric
    available_features = [col for col in feature_cols if col in df_cluster_kinematics.columns and pd.api.types.is_numeric_dtype(df_cluster_kinematics[col])]
    print(f"Using features: {available_features}")

    X = df_cluster_kinematics[available_features]
    y = df_cluster_kinematics['cluster_type'].apply(lambda x: 0 if x == 'Open' else 1) # 0=Open, 1=Globular

    # Optional: Scale features (can improve performance for some models)
    # scaler = StandardScaler()
    # X_scaled = scaler.fit_transform(X)
    # X = pd.DataFrame(X_scaled, columns=available_features, index=X.index) # Use scaled data

    # --- >>> ADD DEBUGGING LINES HERE <<< ---
    print("\n--- DEBUG: Inspecting data BEFORE train_test_split ---")
    print("df_cluster_kinematics[['cluster_name', 'cluster_type']]:")
    # Use options to display all rows if needed (for 12 clusters it should be fine)
    with pd.option_context('display.max_rows', None, 'display.max_columns', None):
        print(df_cluster_kinematics[['cluster_name', 'cluster_type']])

    print("\nValue counts for 'y' variable (0=Open, 1=Globular):")
    print(y.value_counts())
    print("--- END DEBUG ---")
    # --- >>> END DEBUGGING LINES <<< ---

    # Split data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=132, stratify=y
    )

    print(f"Training set size: {len(X_train)} clusters")
    print(f"Testing set size: {len(X_test)} clusters")
    print(f"Class distribution in training set:\n{y_train.value_counts(normalize=True)}")
    print(f"Class distribution in test set:\n{y_test.value_counts(normalize=True)}")


    # Initialize and train the Random Forest Classifier
    print("\nTraining Random Forest Classifier...")
    rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')
    rf_classifier.fit(X_train, y_train)
    print("Training complete.")

    # --- 3. Evaluation ---
    print("\n--- Evaluating Model ---")
    y_pred = rf_classifier.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print(f"Model Accuracy: {accuracy:.4f}")

    print("\nClassification Report:")
    try:
        # Get unique classes present in the test set
        unique_classes = sorted(np.unique(y_test))
        # Only use target_names for classes that are actually present
        present_labels = []
        if 0 in unique_classes:
            present_labels.append('Open')
        if 1 in unique_classes:
            present_labels.append('Globular')
        
        print(classification_report(y_test, y_pred, target_names=present_labels, zero_division=0))
    except Exception as e:
        print(f"Could not generate classification report: {e}")
        print(f"Test set only contains classes: {np.unique(y_test)}")
        print(f"Predicted classes: {np.unique(y_pred)}")

    print("\nConfusion Matrix:")
    cm = confusion_matrix(y_test, y_pred)
    print(cm)
    try:
        plt.figure(figsize=(6, 4))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Open', 'Globular'], yticklabels=['Open', 'Globular'])
        plt.xlabel('Predicted Label')
        plt.ylabel('True Label')
        plt.title('Confusion Matrix')
        plt.savefig('confusion_matrix.png') # Save the plot
        plt.show()
    except Exception as e:
        print(f"Could not generate confusion matrix plot: {e}")


    print("\nFeature Importances:")
    importances = rf_classifier.feature_importances_
    feature_importance_df = pd.DataFrame({'Feature': available_features, 'Importance': importances})
    feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)
    print(feature_importance_df)
    try:
        plt.figure(figsize=(10, 6))
        sns.barplot(x='Importance', y='Feature', data=feature_importance_df)
        plt.title('Feature Importances for Cluster Classification')
        plt.tight_layout()
        plt.savefig('feature_importance.png') # Save the plot
        plt.show()
    except Exception as e:
        print(f"Could not generate feature importance plot: {e}")

else:
    print("\nCannot proceed with Machine Learning: Not enough cluster data processed or available (need at least 2).")

print("\nML Classification Script Finished.")